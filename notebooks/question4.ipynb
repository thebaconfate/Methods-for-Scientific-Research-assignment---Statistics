{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 64\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "from math import ceil\n",
    "power_analysis = TTestIndPower()\n",
    "sample_size = ceil(power_analysis.solve_power(effect_size=0.5, power=0.8, alpha=0.05))\n",
    "print(f'Sample Size: {sample_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment count: 1000\n",
      "Reject count: 0\n",
      "Type 1 error rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "sig = 0.05\n",
    "reject_count = 0\n",
    "experiment_count = 0\n",
    "\n",
    "def generate_other_sample(sample):\n",
    "    new_sample = rng.uniform(size=sample_size)\n",
    "    new_sample = new_sample - new_sample.mean()  # Center the sample around its mean\n",
    "    new_sample = sample.mean() + (sample.std() / new_sample.std()) * new_sample  # Scale to have the same std as sample1\n",
    "    new_sample = new_sample + (sample.mean() - new_sample.mean())  # Adjust the mean to match sample1\n",
    "    return new_sample\n",
    "\n",
    "def generate_samples(sample_size):\n",
    "    sample1 = rng.uniform(0, 1, sample_size)\n",
    "    sample2 = generate_other_sample(sample1)\n",
    "    return sample1, sample2\n",
    "\n",
    "def perform_ttest(sample_size, sig):\n",
    "    s1, s2 = generate_samples(sample_size)\n",
    "    _, p = ttest_ind(s1, s2)\n",
    "    return p < sig\n",
    "\n",
    "def perform_experiment(sample_size, sig, test):\n",
    "    global experiment_count\n",
    "    experiment_count += 1\n",
    "    if test(sample_size, sig):\n",
    "        global reject_count\n",
    "        reject_count += 1\n",
    "\n",
    "for _ in range(1000):\n",
    "    perform_experiment(sample_size, sig, perform_ttest)\n",
    "print(f'Experiment count: {experiment_count}')\n",
    "print(f'Reject count: {reject_count}')\n",
    "print(f'Type 1 error rate: {reject_count / experiment_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment count: 1000\n",
      "Reject count: 1000\n",
      "Power: 1.0\n"
     ]
    }
   ],
   "source": [
    "effect_size = 0.5\n",
    "experiment_count = 0\n",
    "reject_count = 0\n",
    "\n",
    "def perform_ttest_with_effect_size(sample_size, sig):\n",
    "    s1 = rng.uniform(0,1,sample_size)\n",
    "    s2 = rng.uniform(0,1,sample_size)\n",
    "    s2 += effect_size\n",
    "    _, p = ttest_ind(s1, s2)\n",
    "    return p < sig\n",
    "\n",
    "for _ in range(1000):\n",
    "    perform_experiment(sample_size, sig, perform_ttest_with_effect_size)\n",
    "print(f'Experiment count: {experiment_count}')\n",
    "print(f'Reject count: {reject_count}')\n",
    "print(f'Power: {reject_count / experiment_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment count: 1000\n",
      "Reject count: 0\n",
      "Type 1 error rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "experiment_count = 0\n",
    "reject_count = 0\n",
    "\n",
    "def generate_cauchy_samples(sample_size):\n",
    "    cauchy1 = rng.standard_cauchy(size=sample_size)\n",
    "    cauchy2 = rng.standard_cauchy(size=sample_size)\n",
    "    cauchy2 = cauchy2 - cauchy2.mean()  # Center the sample around its mean\n",
    "    cauchy2 = cauchy1.mean() + (cauchy1.std() / cauchy2.std()) * cauchy2  # Scale to have the same std as sample1\n",
    "    cauchy2 = cauchy2 + (cauchy1.mean() - cauchy2.mean())  # Adjust the mean to match sample1\n",
    "    return cauchy1, cauchy2\n",
    "\n",
    "def cauchy_ttest(sample_size, sig):\n",
    "    c1, c2 = generate_cauchy_samples(sample_size)\n",
    "    _, p = ttest_ind(c1, c2)\n",
    "    return p < sig\n",
    "pool = Pool()\n",
    "for _ in range(1000):\n",
    "    pool.apply_async(perform_experiment, args=(sample_size, sig, cauchy_ttest))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(f'Experiment count: {experiment_count}')\n",
    "print(f'Reject count: {reject_count}')\n",
    "print(f'Type 1 error rate: {reject_count / experiment_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment count: 1000\n",
      "Reject count: 29\n",
      "Power: 0.029\n"
     ]
    }
   ],
   "source": [
    "reject_count = 0\n",
    "experiment_count = 0\n",
    "effect_size = 0.5\n",
    "\n",
    "def cauchy_ttest_effect_size(sample_size, sig):\n",
    "    c1, c2 = rng.standard_cauchy(size=sample_size), rng.standard_cauchy(size=sample_size)\n",
    "    c2 += effect_size\n",
    "    _, p = ttest_ind(c1, c2)\n",
    "    return p < sig\n",
    "\n",
    "pool = Pool()\n",
    "for _ in range(1000):\n",
    "    pool.apply_async(perform_experiment, args=(sample_size, sig, cauchy_ttest_effect_size))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(f'Experiment count: {experiment_count}')\n",
    "print(f'Reject count: {reject_count}')\n",
    "print(f'Power: {reject_count / experiment_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 (2) wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment count: 1000\n",
      "Reject count: 317\n",
      "Power: 0.317\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "experiment_count = 0\n",
    "reject_count = 0\n",
    "\n",
    "def cauchy_mannwhitneyu(sample_size, sig):\n",
    "    c1, c2 = rng.standard_cauchy(size=sample_size), rng.standard_cauchy(size=sample_size)\n",
    "    c2 += effect_size\n",
    "    _, p = mannwhitneyu(c1, c2)\n",
    "    return p < sig\n",
    "pool = Pool()\n",
    "for _ in range(1000):\n",
    "    pool.apply_async(perform_experiment, args=(sample_size, sig, cauchy_mannwhitneyu))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(f'Experiment count: {experiment_count}')\n",
    "print(f'Reject count: {reject_count}')\n",
    "print(f'Power: {reject_count / experiment_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
